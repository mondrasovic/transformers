{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Milos/slovak-gpt-j-162M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Milos/slovak-gpt-j-162M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Moje najobľubenejšie mesto na severe Slovenska je\"\n",
    "encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**encoded_input)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import pathlib\n",
    "from typing import Dict, Iterable, Tuple, Any\n",
    "\n",
    "class MessageFilter:\n",
    "    def __init__(self, *message_json_file_paths: Tuple[str, ...]) -> None:\n",
    "        self.message_dumps = []\n",
    "        for message_json_file_path in message_json_file_paths:\n",
    "            with open(message_json_file_path, \"rt\") as in_file:\n",
    "                message_dump = json.load(in_file)\n",
    "                self.message_dumps.append(message_dump)\n",
    "\n",
    "    def __call__(self, sender_name: str) -> Iterable[str]:\n",
    "        for message_dump in self.message_dumps:\n",
    "            yield from self._find_messages_from(message_dump, sender_name)\n",
    "\n",
    "    def _find_messages_from(self, messages_dump: Dict[str, Any], sender_name: str) -> Iterable[str]:\n",
    "        for message in messages_dump[\"messages\"]:\n",
    "            content = message.get(\"content\")\n",
    "            if content is None:\n",
    "                continue\n",
    "            curr_sender_name = self._fix_encoding(message[\"sender_name\"])\n",
    "            if curr_sender_name == sender_name:\n",
    "                message_text = self._fix_encoding(message[\"content\"])\n",
    "                message_text_processed = self._process_message(message_text)\n",
    "                if message_text_processed:\n",
    "                    yield message_text_processed\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fix_encoding(text: str) -> str:\n",
    "        return text.encode(\"latin1\").decode(\"utf-8\")\n",
    "    \n",
    "    @classmethod\n",
    "    def _process_message(cls, text: str) -> str:\n",
    "        text = cls.remove_urls(text)\n",
    "        text = cls.remove_accents(text)\n",
    "        text = cls.normalize_whitespaces(text)\n",
    "        text = cls.normalize_emojis(text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_urls(text: str) -> str:\n",
    "        return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_accents(text: str) -> str:\n",
    "        nfkd_form = unicodedata.normalize(\"NFKD\", text)\n",
    "        only_ascii = nfkd_form.encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "        return only_ascii\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_whitespaces(text: str) -> str:\n",
    "        return re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_emojis(text: str) -> str:\n",
    "        replacements = (\n",
    "            (\":-D\", \":D\"),\n",
    "            (\":-)\", \":)\"),\n",
    "            (\":-(\", \":(\"),\n",
    "            (\":-P\", \":P\"),\n",
    "            (\":-*\", \":*\")\n",
    "        )\n",
    "        for old_sequence, new_sequence in replacements:\n",
    "            text = text.replace(old_sequence, new_sequence)\n",
    "        return text\n",
    "\n",
    "message_json_file_paths = [str(file) for file in pathlib.Path(\"/mnt/e/FB_data/messages\").rglob(\"*.json\")]\n",
    "\n",
    "message_filter = MessageFilter(*message_json_file_paths)\n",
    "messages = list(message_filter(\"Milan Ondrašovič\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../datasets/facebook_messages/messages.txt\", \"wt\") as out_file:\n",
    "    out_file.write(\"\\n\".join(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(messages)\n",
    "lengths = [len(message.split()) for message in messages]\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data={\"lengths\": lengths})\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Milos/slovak-gpt-j-162M\")\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"../../datasets/facebook_messages/messages.txt\",\n",
    "    block_size=64\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=500,\n",
    "#     save_steps=500,\n",
    "#     num_train_epochs=10,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     logging_steps=100,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"saved_model/mond/slovak-fb-msg-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.train(files=[\"../../datasets/facebook_messages/messages.txt\"])\n",
    "# tokenizer.save_pretrained(\"pretrained_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a new tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Train the tokenizer on a corpus of text\n",
    "tokenizer.train(\n",
    "    files=[\"../../datasets/facebook_messages/messages.txt\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "        special_tokens=[\n",
    "        \"<s>\", \"<pad>\", \"</s>\", \"<unk>\",\n",
    "    ]\n",
    ")\n",
    "tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
    "text = \"Ja neviem co by som tu napisal, ale hadam to pojde!\"\n",
    "\n",
    "tokenized_text = tokenizer.encode(text)\n",
    "tokenized_text\n",
    "generated_text = tokenizer.decode(tokenized_text, skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, GPT2Config\n",
    ")\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "vocab_size = 32768\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=[\"../../datasets/facebook_messages/messages.txt\"],\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "        special_tokens=[\n",
    "        \"<s>\", \"<pad>\", \"</s>\", \"<unk>\",\n",
    "    ]\n",
    ")\n",
    "tokenizer.save_model(\"tokenizer\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
    "\n",
    "config = GPT2Config.from_pretrained(\n",
    "    \"gpt2\", vocab_size=len(tokenizer), n_positions=256, n_embd=512, n_layer=6, n_head=4, n_inner=1024\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"../../datasets/facebook_messages/messages.txt\",\n",
    "    block_size=64\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = dataset.with_format(\"torch\", device=device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.save_model(\"saved_model/mond/slovak-fb-msg-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"saved_model/mond/slovak-fb-msg-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 09:30:42.987152: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-02 09:30:44.107467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-02 09:30:44.107582: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-02 09:30:44.107588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29,526,528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, GPT2Config\n",
    ")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"saved_model/mond/slovak-fb-msg-gpt2\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
    "\n",
    "print(f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veeela lubim! No ako sa mas? :D\n",
      "Uz pojdem dat na krku a ked sa zobudis. :D No k nej prides, potom vecer.\n",
      "Uz som vstal na chvilku, chcel som taku pol piatej :D Uz som nufik zavolat, dnes tiez mi z toho bol pekne. :D <3\n",
      "Nazdar, tesim. Som rad. Vraj z ruk je normalne. :D Uvidis, ze si to aj trosku mozes zacat prist, keby uz. Rano som zistil, ze je podla mna. Vsak aj vyzera to asi problem trosku, nemam. :D Tesim sa za chvilku. :*\n",
      "No ved to, tam je na oboch. Idem ja na obed 3 mesiace, a potom cvicit, tak potom vediet a pojdem aj na vecer, a tak som tam aj na FB uz povedal. :D Potom o 15:00 sa to da. No moooje. Veeela. :*\n",
      "Videocet sa skoncil.\n",
      "Videocet sa skoncil.\n",
      "Moj nufik! O tej osmej, dobre, ako sa mas? Ako ma? :D Za chvilku <3\n",
      "Aj ja teba. :D Este som dnes nufik to mal. :* <3 Uta!\n",
      "V stave je ina stranka na par minut v pohode, ale ked je to tak sa ti o tom mne lepsie. :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text with the model\n",
    "input_text = \"Veeela lubim! No ako sa mas? :D\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=128, do_sample=True, top_k=128)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
