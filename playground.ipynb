{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Milos/slovak-gpt-j-162M\")\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"../../datasets/facebook_messages/messages.txt\",\n",
    "    block_size=64\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=500,\n",
    "#     save_steps=500,\n",
    "#     num_train_epochs=10,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     logging_steps=100,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"saved_model/mond/slovak-fb-msg-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a new tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Train the tokenizer on a corpus of text\n",
    "tokenizer.train(\n",
    "    files=[\"../../datasets/facebook_messages/messages.txt\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "        special_tokens=[\n",
    "        \"<s>\", \"<pad>\", \"</s>\", \"<unk>\",\n",
    "    ]\n",
    ")\n",
    "tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, GPT2Config\n",
    ")\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "vocab_size = 32768\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=[\"../../datasets/facebook_messages/messages.txt\"],\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "        special_tokens=[\n",
    "        \"<s>\", \"<pad>\", \"</s>\", \"<unk>\",\n",
    "    ]\n",
    ")\n",
    "tokenizer.save_model(\"tokenizer\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
    "\n",
    "config = GPT2Config.from_pretrained(\n",
    "    \"gpt2\", vocab_size=len(tokenizer), n_positions=256, n_embd=512, n_layer=6, n_head=4, n_inner=1024\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"../../datasets/facebook_messages/messages.txt\",\n",
    "    block_size=64\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = dataset.with_format(\"torch\", device=device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.save_model(\"saved_model/mond/slovak-fb-msg-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"saved_model/mond/slovak-fb-msg-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, GPT2Config\n",
    ")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"saved_model/mond/slovak-fb-msg-gpt2\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
    "\n",
    "print(f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the model\n",
    "input_text = \"Veeela lubim! No ako sa mas? :D\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=128, do_sample=True, top_k=128)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidTextSampleFile",
     "evalue": "could not load subtitles file /mnt/e/datasets/open_subtitles/sk/2003/421357/3997680.xml",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/transformers/lm/data/reader.py:105\u001b[0m, in \u001b[0;36mSubtitlesReader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubtitles\u001b[39m.\u001b[39;49mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iter_subtitles_from_xml(subtitles_file_path))\n\u001b[1;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m ParseError \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/projects/transformers/lm/data/reader.py:117\u001b[0m, in \u001b[0;36mSubtitlesReader._iter_subtitles_from_xml\u001b[0;34m(subtitles_file_path)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(subtitles_file_path, \u001b[39m\"\u001b[39m\u001b[39mrt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m xml_file:\n\u001b[0;32m--> 117\u001b[0m     root \u001b[39m=\u001b[39m ET\u001b[39m.\u001b[39;49mfromstring(xml_file\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m    119\u001b[0m \u001b[39mfor\u001b[39;00m s_tag \u001b[39min\u001b[39;00m root\u001b[39m.\u001b[39miter(\u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py:1342\u001b[0m, in \u001b[0;36mXML\u001b[0;34m(text, parser)\u001b[0m\n\u001b[1;32m   1341\u001b[0m     parser \u001b[39m=\u001b[39m XMLParser(target\u001b[39m=\u001b[39mTreeBuilder())\n\u001b[0;32m-> 1342\u001b[0m parser\u001b[39m.\u001b[39;49mfeed(text)\n\u001b[1;32m   1343\u001b[0m \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mParseError\u001b[0m: not well-formed (invalid token): line 1356, column 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidTextSampleFile\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m subtitles_reader \u001b[39m=\u001b[39m SubtitlesReader\u001b[39m.\u001b[39mfrom_directories(\u001b[39m\"\u001b[39m\u001b[39m/mnt/e/datasets/open_subtitles/sk\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m dataset_generator \u001b[39m=\u001b[39m PlainTextDatasetGenerator(conversation_preprocessor)\n\u001b[0;32m---> 12\u001b[0m dataset_generator\u001b[39m.\u001b[39;49mgenerate_from_readers(\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/mnt/e/datasets/conversation_corpus/dataset.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, facebook_messages_reader, subtitles_reader\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/projects/transformers/lm/data/dataset.py:16\u001b[0m, in \u001b[0;36mPlainTextDatasetGenerator.generate_from_readers\u001b[0;34m(self, output_file_path, *text_sample_readers)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_file_path, \u001b[39m\"\u001b[39m\u001b[39mwt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m out_file:\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m text_sample_reader \u001b[39min\u001b[39;00m text_sample_readers:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mfor\u001b[39;00m text_sample \u001b[39min\u001b[39;00m text_sample_reader\u001b[39m.\u001b[39mread_samples():\n\u001b[1;32m     17\u001b[0m             text_sample_preprocessed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessor(text_sample)\n\u001b[1;32m     18\u001b[0m             \u001b[39mif\u001b[39;00m text_sample_preprocessed:\n",
      "File \u001b[0;32m~/projects/transformers/lm/data/reader.py:38\u001b[0m, in \u001b[0;36mTextSampleReader.read_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_samples\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_samples_after_load()\n",
      "File \u001b[0;32m~/projects/transformers/lm/data/reader.py:107\u001b[0m, in \u001b[0;36mSubtitlesReader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubtitles\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_subtitles_from_xml(subtitles_file_path))\n\u001b[1;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m ParseError \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidTextSampleFile(\n\u001b[1;32m    108\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcould not load subtitles file \u001b[39m\u001b[39m{\u001b[39;00msubtitles_file_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n",
      "\u001b[0;31mInvalidTextSampleFile\u001b[0m: could not load subtitles file /mnt/e/datasets/open_subtitles/sk/2003/421357/3997680.xml"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lm.data.dataset import PlainTextDatasetGenerator\n",
    "from lm.data.preprocessing import ConversationPreprocessor\n",
    "from lm.data.reader import FacebookMessagesReader, SubtitlesReader\n",
    "\n",
    "conversation_preprocessor = ConversationPreprocessor()\n",
    "facebook_messages_reader = FacebookMessagesReader.from_directories(\"/mnt/e/datasets/FB_messages\")\n",
    "subtitles_reader = SubtitlesReader.from_directories(\"/mnt/e/datasets/open_subtitles/sk\")\n",
    "dataset_generator = PlainTextDatasetGenerator(conversation_preprocessor)\n",
    "dataset_generator.generate_from_readers(\n",
    "    \"/mnt/e/datasets/conversation_corpus/dataset.txt\", facebook_messages_reader, subtitles_reader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c)\n",
    "# gmail\n",
    "# hotmail\n",
    "# .cz\n",
    "# .sk\n",
    "# .com\n",
    "# webzdarma\n",
    "# titulky\n",
    "# T I T U L K Y\n",
    "# preklad\n",
    "# seznam\n",
    "# released\n",
    "# thanks\n",
    "# @\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "692e13bfafb79b4e70c57aa9d2eafccbd1458e3cf6de592b604ccdb87c5dc09f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
